{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MultiGPU.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ULyIoqBxH0z0"},"source":["# Multi-GPU Considerations\n","\n","These days, training neural networks with multiple GPU is often necessary.\n","As SpeechBrain is strongly linked to PyTorch, we provide the same multi-GPU utilities:\n","\n","- *Data Parallel (DP)*\n","- *Distributed Data Parallel (DDP)*.\n","\n","One big difference between DP and DDP is that DP can only be run on a single machine (with multiple GPUs), while  DDP can exploit GPUs across different servers as well. \n"]},{"cell_type":"markdown","metadata":{"id":"giZzencLWy75"},"source":["## DataParallel\n","Data Parallel (DP) relies on a wrapper applied to neural network modules.\n","The wrapper can be simpy applied following:\n"," \n","`nn_modules = torch.nn.DataParallel(nn_modules).`\n","\n","DP uses 4 primitives to implement data parallelism:\n","1. *replicate:* replicates a Module on multiple GPU devices.\n","2. *scatter:* distributes the input in the first_dimension\n","3. *parallel_apply:* applies a set of already-distributed inputs to a set of already-distributed models.\n","4. *gather:* gathers and concatenate the outputs.\n","\n","\n","The common pattern for using Data Parallel in SpeechBrain is the following:\n","\n","\n","```\n","cd recipes/<dataset>/<task>/\n","python experiment.py params.yaml --data_parallel_backend=True --data_parallel_count=2\n","```\n","\n","**IMPORTANT**: the batch size for each GPU process is: **batch_size / data_parallel_count**. So you should consider changing the batch_size value according to your need."]},{"cell_type":"markdown","metadata":{"id":"ck1qe3KvpZPV"},"source":["## Distributed Data Parallel (DDP)\n","\n","DDP implements data parallelism on different processes. This way, the GPUs do not necessarily have to be in the same server. This solution is much more flexible. However, the training routines must be written considering multi-threading. \n","\n","With SpeechBrain, we put several efforts to make sure the code is compliant with DDP. For instance, to avoid conflicts across processes we develop the `run_on_main` function. It is called when critical operations such as writing a file on disk are performed. It ensures that these operations are run in a single process only. The other processes are waiting until this operation is completed.\n","\n","Using DDP in speechbrain is quite easy:\n","\n","```\n","cd recipes/<dataset>/<task>/\n","python -m torch.distributed.launch --nproc_per_node=4 experiment.py hyperparams.yaml --distributed_launch=True --distributed_backend='nccl'\n","```\n","\n","Where:\n","- nproc_per_node must be equal to the number of GPUs.\n","- distributed_backend is the type of backend managing multiple processes synchronizations (e.g, 'nccl', 'gloo'). Try to switch the DDP backend if you have issues with nccl.\n","\n","You can run the model in different servers with:\n","\n","\n","```\n","# Machine 1\n","cd recipes/<dataset>/<task>/\n","python -m torch.distributed.launch --nproc_per_node=2 --nnodes=2 --node=0 --master_addr machine_1_adress --master_port 5555 experiment.py hyperparams.yaml --distributed_launch=True --distributed_backend='nccl'\n","\n","# Machine 2\n","cd recipes/<dataset>/<task>/\n","python -m torch.distributed.launch --nproc_per_node=2 --nnodes=2 --node=1 --master_addr machine_1_adress --master_port 5555 experiment.py hyperparams.yaml --distributed_launch=True --distributed_backend='nccl'\n","\n","Machine 1 will have 2 subprocess (subprocess1: with local_rank=0, rank=0, and subprocess2: with local_rank=1, rank=1). Machine 2 will have 2 subprocess (subprocess1: with local_rank=0, rank=2, and subprocess2: with local_rank=1, rank=3).\n","```\n","\n","\n","To use DDP, you should consider using `torch.distributed.launch` for setting the subprocess with the right Unix variables (`local_rank` and `rank`). The `local_rank` variable enables the program to set the right device argument for each DDP subprocess, while the `rank` variable (which is unique for each subprocess) will be used for registering the subprocess rank to the DDP group. In this way, we can manage multi-GPU training over multiple servers.\n","\n","Note that using DDP on different machines introduces a **communication overhead** that might slow down training (depending on how fast is the connection across the different machines). \n","\n","\n","We would like to advise our users that despite being more efficient, DDP is also more prone to exhibit unexpected bugs. Indeed, DDP is quite server-dependent and some setups might generate errors with the PyTorch implementation of DDP.  The future version of pytorch will improve the stability of DDP.\n","\n","\n"]}]}